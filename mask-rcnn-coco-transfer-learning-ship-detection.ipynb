{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Coasties Ship Detection using NOAA National Geodetic Survey Tampa Image Data Set\n## JAIC Create AI Jan 2022 Cohort \n#### Students\n* Sydney Wagner\n* Amilynn Adams\n* Evan Twarog\n* David Couture\n\n#### Facilitator\n* Bruce Goldfeder\n\n## Presentation Date April 22, 2022\n* Ship Object Detection using pre-trained COCO weights trained on http://cocodataset.org as in https://github.com/matterport/Mask_RCNN/tree/master/samples/balloon\n* Forked from Henri Mendoca Kaggle Notebook - https://www.kaggle.com/code/hmendonca/airbus-mask-rcnn-and-coco-transfer-learning\n\n\n","metadata":{}},{"cell_type":"code","source":"#delete me after code is ported externally\n\n#!pip freeze > requirements.txt\n#!cat requirements.txt\n# This code downloads the Kaggle local file to your system\n#from IPython.display import FileLink\n#FileLink(r'requirements.txt')","metadata":{"_kg_hide-output":true,"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-14T17:31:38.795251Z","iopub.execute_input":"2022-04-14T17:31:38.795608Z","iopub.status.idle":"2022-04-14T17:31:38.800566Z","shell.execute_reply.started":"2022-04-14T17:31:38.795554Z","shell.execute_reply":"2022-04-14T17:31:38.799843Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"#delete me after code is ported externally\n\n#!pwd\n#!ls -al","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-14T17:31:38.802324Z","iopub.execute_input":"2022-04-14T17:31:38.802916Z","iopub.status.idle":"2022-04-14T17:31:38.811504Z","shell.execute_reply.started":"2022-04-14T17:31:38.802868Z","shell.execute_reply":"2022-04-14T17:31:38.810712Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# Commiting for submission\n#debug = False\ndebug = True","metadata":{"_uuid":"cdb40bf9115f53810c9e13f0a50e53ed9eb6221b","_kg_hide-input":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-04-14T17:31:38.815399Z","iopub.execute_input":"2022-04-14T17:31:38.816080Z","iopub.status.idle":"2022-04-14T17:31:38.820210Z","shell.execute_reply.started":"2022-04-14T17:31:38.816024Z","shell.execute_reply":"2022-04-14T17:31:38.819281Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"# Step One - Setup\n1. Import required libraries\n2. Declare constants for data and output folders\n3. Import Matterport Mask-RCNN\n4. Download Pretrained COCO weights\n5. Create Configuration for model parameters\n6. Create Utility functions for manipulating image and mask data\n","metadata":{}},{"cell_type":"markdown","source":"* ### 1.1 Import python libraries","metadata":{}},{"cell_type":"code","source":"import os \nimport sys\nimport random\nimport math\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport json\nfrom imgaug import augmenters as iaa\nfrom tqdm import tqdm\nimport pandas as pd \nimport glob \nfrom skimage.io import imread\nfrom matplotlib.cm import get_cmap\nfrom skimage.segmentation import mark_boundaries\nfrom skimage.util import montage\nfrom skimage.morphology import binary_opening, disk, label\nimport gc; gc.enable() # memory is tight\nfrom tabulate import tabulate","metadata":{"id":"4kjcC6QqywWl","_uuid":"40c67b3ff0fa04587dec508363308adaa3ceaf34","execution":{"iopub.status.busy":"2022-04-14T17:31:38.822504Z","iopub.execute_input":"2022-04-14T17:31:38.823141Z","iopub.status.idle":"2022-04-14T17:31:38.832185Z","shell.execute_reply.started":"2022-04-14T17:31:38.823087Z","shell.execute_reply":"2022-04-14T17:31:38.831525Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"markdown","source":"* ### 1.2 Declare constants for folder locations","metadata":{}},{"cell_type":"code","source":"# Location for kaggle data for airbus ship detection\nDATA_DIR = '/kaggle/input/airbus-ship-detection'\n\n# Directory to save logs and trained model\nROOT_DIR = '/kaggle/working'\n\n# Location for train and test data\ntrain_dir = os.path.join(DATA_DIR, 'train_v2')\ntest_dir = os.path.join(DATA_DIR, 'test_v2')","metadata":{"id":"yP0XLJx_x_6o","_uuid":"6e5764759e6a0a9b698b44645658f66873edd807","execution":{"iopub.status.busy":"2022-04-14T17:31:38.833223Z","iopub.execute_input":"2022-04-14T17:31:38.835394Z","iopub.status.idle":"2022-04-14T17:31:38.842948Z","shell.execute_reply.started":"2022-04-14T17:31:38.835349Z","shell.execute_reply":"2022-04-14T17:31:38.842317Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"* ### 1.3 Install Matterport's Mask-RCNN model from github.\n    * See the [Matterport's implementation of Mask-RCNN](https://github.com/matterport/Mask_RCNN).","metadata":{"id":"kdYzLq1zfKL4","_uuid":"576df4c47a23d08b1bdb384245e09aa69f88bbd3"}},{"cell_type":"code","source":"# Download the Mask-RCNN from Github\n!git clone https://www.github.com/matterport/Mask_RCNN.git\nos.chdir('Mask_RCNN')\n\n# Import Mask RCNN\nsys.path.append(os.path.join(ROOT_DIR, 'Mask_RCNN'))  # To find local version of the library\nfrom mrcnn.config import Config\nfrom mrcnn import utils\nimport mrcnn.model as modellib\nfrom mrcnn import visualize\nfrom mrcnn.model import log","metadata":{"id":"KgllzLnDr7kF","outputId":"6c978df7-2013-437e-acd1-5011048dfb53","_uuid":"b37d22551d332f0f7b722cc7204eb614524b6c21","execution":{"iopub.status.busy":"2022-04-14T17:31:38.843952Z","iopub.execute_input":"2022-04-14T17:31:38.844184Z","iopub.status.idle":"2022-04-14T17:31:46.138021Z","shell.execute_reply.started":"2022-04-14T17:31:38.844143Z","shell.execute_reply":"2022-04-14T17:31:46.137083Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"* ### 1.4 Download COCO pre-trained weights","metadata":{"_uuid":"f108beef7838be8a64dd512d395c5dc0ad952790"}},{"cell_type":"code","source":"# Download the weights\n!wget --quiet https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5\n!ls -lh mask_rcnn_coco.h5\n\n# Set constant for path to weights\nCOCO_WEIGHTS_PATH = \"mask_rcnn_coco.h5\"","metadata":{"_uuid":"c3ee0cd0ee0b1defdec97b94bc736587c1f7631f","execution":{"iopub.status.busy":"2022-04-14T17:31:46.139132Z","iopub.execute_input":"2022-04-14T17:31:46.139410Z","iopub.status.idle":"2022-04-14T17:32:03.869289Z","shell.execute_reply.started":"2022-04-14T17:31:46.139363Z","shell.execute_reply":"2022-04-14T17:32:03.868492Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"* ### 1.5 Configuration paramters for Mask-RCNN\n\n    * Hardware based configurations \n    * Architecture based configurations\n    * Mask-RCNN algorithm based configurations","metadata":{"id":"gj-tvDvEaDiC","_uuid":"032cc5fe4baa051108106675e6ca4f4fdb2846ed"}},{"cell_type":"code","source":"# The following parameters have been selected to reduce running time for demonstration purposes \n# These are not optimal \n\nclass DetectorConfig(Config):    \n    # Give the configuration a recognizable name  \n    NAME = 'airbus'\n    \n    # Hardware based config\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 9\n    \n    # Architecture based config\n    BACKBONE = 'resnet50'    \n    NUM_CLASSES = 2  # background and ship classes\n    \n    # Algorithm based config\n    IMAGE_MIN_DIM = 384\n    IMAGE_MAX_DIM = 384\n    RPN_ANCHOR_SCALES = (8, 16, 32, 64)\n    TRAIN_ROIS_PER_IMAGE = 64\n    MAX_GT_INSTANCES = 14\n    DETECTION_MAX_INSTANCES = 10\n    DETECTION_MIN_CONFIDENCE = 0.95\n    DETECTION_NMS_THRESHOLD = 0.0\n    \n    STEPS_PER_EPOCH = 15 if debug else 150\n    VALIDATION_STEPS = 10 if debug else 125\n    \n    ## balance out losses\n    LOSS_WEIGHTS = {\n        \"rpn_class_loss\": 30.0,\n        \"rpn_bbox_loss\": 0.8,\n        \"mrcnn_class_loss\": 6.0,\n        \"mrcnn_bbox_loss\": 1.0,\n        \"mrcnn_mask_loss\": 1.2\n    }\n\nconfig = DetectorConfig()\nconfig.display()","metadata":{"id":"_SfzTa-1zOck","outputId":"91ae8935-bccb-4b8e-9a7e-aa690f95fd9b","_uuid":"dfcffc4eaa94a41497717851dee9f702d8a2a73b","execution":{"iopub.status.busy":"2022-04-14T17:32:03.870447Z","iopub.execute_input":"2022-04-14T17:32:03.870721Z","iopub.status.idle":"2022-04-14T17:32:03.891587Z","shell.execute_reply.started":"2022-04-14T17:32:03.870674Z","shell.execute_reply":"2022-04-14T17:32:03.890740Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"**Train: 192556 (768,768,3)**  \n**Test :  15606 (768,768,3)**","metadata":{}},{"cell_type":"markdown","source":"* ### 1.6 Utility functions for image and mask data manipulation\n    * Making montages from several images \n    * Run Length Encoding (RLE) decoding and encoding to parse the RLE formatted masking data in the csv data files\n    * Converting mask data into images for viewing","metadata":{}},{"cell_type":"code","source":"# Function for creating montages of several images\nmontage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1) # to rgb function\n\n# RLE encode and decode functions\n# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\ndef multi_rle_encode(img, **kwargs):\n    '''\n    Encode connected regions as separated masks\n    '''\n    labels = label(img)\n    if img.ndim > 2:\n        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n    else:\n        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n\n\ndef rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None): # raster to rle coding \n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    if np.max(img) < min_max_threshold:\n        return '' ## no need to encode if it's all zeros\n    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n        return '' ## ignore overfilled mask\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef rle_decode(mask_rle, shape=(768, 768)): # rle coding to raster\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (height,width) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T  # Needed to align to RLE direction\n\n# Mask image creation black and white and color for mulitple ships for viewing\ndef masks_as_image(in_mask_list):\n    # Take the individual ship masks and create a single mask array for all ships\n    all_masks = np.zeros((768, 768), dtype = np.uint8)\n    for mask in in_mask_list:\n        if isinstance(mask, str):\n            all_masks |= rle_decode(mask)\n    return all_masks\n\ndef masks_as_color(in_mask_list):\n    # Take the individual ship masks and create a color mask array for each ships\n    all_masks = np.zeros((768, 768), dtype = np.float)\n    scale = lambda x: (len(in_mask_list)+x+1) / (len(in_mask_list)*2) ## scale the heatmap image to shift \n    for i,mask in enumerate(in_mask_list):\n        if isinstance(mask, str):\n            all_masks[:,:] += scale(i) * rle_decode(mask)\n    return all_masks","metadata":{"_uuid":"6136132b1f1b311e297d9432772ec4a81230924f","execution":{"iopub.status.busy":"2022-04-14T17:32:03.892619Z","iopub.execute_input":"2022-04-14T17:32:03.892891Z","iopub.status.idle":"2022-04-14T17:32:03.912466Z","shell.execute_reply.started":"2022-04-14T17:32:03.892847Z","shell.execute_reply":"2022-04-14T17:32:03.911504Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"# Step Two - Data Exploration and Cleaning\n* ### Read in and view the image mask data in RLE format\n    * RLE is run-length encoding. It is used to encode the location of foreground objects in segmentation. Instead of outputting a mask image, you give a list of start pixels and how many pixels after each of those starts is included in the mask.","metadata":{}},{"cell_type":"code","source":"# Read in the mask data from the training dataset\nSEGMENTATION = DATA_DIR + '/train_ship_segmentations_v2.csv'\nanns = pd.read_csv(SEGMENTATION)\n\n# Display the first five rows first is truncated second is full RLE listing for mask\nprint(anns[:5])\nprint('\\nSame table with full listing for RLE encoded pixels\\n')\nprint(tabulate(anns[:5],headers='firstrow'))\n","metadata":{"_uuid":"3050fa77026411ffdc27bed4a9b667ec0467e4ce","execution":{"iopub.status.busy":"2022-04-14T17:32:03.917990Z","iopub.execute_input":"2022-04-14T17:32:03.918602Z","iopub.status.idle":"2022-04-14T17:32:04.537648Z","shell.execute_reply.started":"2022-04-14T17:32:03.918553Z","shell.execute_reply":"2022-04-14T17:32:04.536801Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":"* ### Remove all the junk images\n    * Cleaning the data by removing all invalid images from the train and test data sets\n    * These were provided by forked version","metadata":{}},{"cell_type":"code","source":"# Note that this step may not be needed as the training only uses positive training of images with ships\n# None of the images that are unreadable have any ships on them\n\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\nexclude_list = ['6384c3e78.jpg','13703f040.jpg', '14715c06d.jpg',  '33e0ff2d5.jpg',\n                '4d4e09f2a.jpg', '877691df8.jpg', '8b909bb20.jpg', 'a8d99130e.jpg', \n                'ad55c3143.jpg', 'c8260c541.jpg', 'd6c7f17c7.jpg', 'dc3e7c901.jpg',\n                'e44dffe88.jpg', 'ef87bad36.jpg', 'f083256d8.jpg'] #corrupted images\n\ntrain_names = [f for f in os.listdir(train_dir) if f not in exclude_list]\ntest_names = [f for f in os.listdir(test_dir) if f not in exclude_list]\n\nprint(len(train_names), len(test_names))","metadata":{"_uuid":"d3e05fa1a38c637fa228acd62b92dd41117a6672","execution":{"iopub.status.busy":"2022-04-14T17:32:04.541171Z","iopub.execute_input":"2022-04-14T17:32:04.541649Z","iopub.status.idle":"2022-04-14T17:32:04.772067Z","shell.execute_reply.started":"2022-04-14T17:32:04.541584Z","shell.execute_reply":"2022-04-14T17:32:04.771222Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":"* ### Create the Train and Validation data sets (called test)","metadata":{}},{"cell_type":"code","source":"# Note - this value is overwriting the assignment in above cell, why? None of them have ships in them so OK\ntrain_names = anns[anns.EncodedPixels.notnull()].ImageId.unique().tolist()  ## override with ships\nprint('Are bad images in files with ships? ',exclude_list in train_names)\nprint(\"Uniq image list containing ships:\",np.shape(train_names))\n\ntest_size = config.VALIDATION_STEPS * config.IMAGES_PER_GPU # 10*9\nprint('Test Size is ', test_size)\nimage_fps_train, image_fps_val = train_test_split(train_names, test_size=test_size, random_state=42)\n\nif debug:\n    image_fps_train = image_fps_train[:100]\n    image_fps_val = image_fps_val[:100]\n    test_names = test_names[:100]\n    \nprint(\"train:\",len(image_fps_train), \"val:\",len(image_fps_val), \"test:\",len(test_names))\n\n# TODO - not sure why this is here\nimage_fps, image_annotations = train_names, anns\nprint(\"Uniq image list containing ships:\",np.shape(image_fps))\nprint(\"Train ship segmentations csv file:\",np.shape(image_annotations))","metadata":{"_uuid":"904636402355a305f7b2ccacb8cc55d52151d2e6","execution":{"iopub.status.busy":"2022-04-14T17:32:04.773354Z","iopub.execute_input":"2022-04-14T17:32:04.773919Z","iopub.status.idle":"2022-04-14T17:32:04.875586Z","shell.execute_reply.started":"2022-04-14T17:32:04.773868Z","shell.execute_reply":"2022-04-14T17:32:04.874804Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":"* ### Class for loading and viewing the image and mask data combined\n    * Encapsulation in a class makes this set of data reads and functions easier to use","metadata":{}},{"cell_type":"code","source":"class DetectorDataset(utils.Dataset):\n    \"\"\"\n    Dataset class for training our dataset.\n    This class inherits from the MRCNN.utils.Dataset class and is used for preparing\n    data to be input into a Matterport Mask-RCNN model for training and inference\n    https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/utils.py \n    \"\"\"\n\n    def __init__(self, image_fps, image_annotations, orig_height, orig_width):\n        super().__init__(self)\n        \n        # Add classes\n        self.add_class('ship', 1, 'Ship')\n        \n        # add images by iterating through the files names getting index and filename\n        for i, fp in enumerate(image_fps):\n            annotations = image_annotations.query('ImageId==\"' + fp + '\"')['EncodedPixels']\n            self.add_image('ship', image_id=i, path=os.path.join(train_dir, fp), \n                           annotations=annotations, orig_height=orig_height, orig_width=orig_width)\n            \n    def image_reference(self, image_id):\n        info = self.image_info[image_id]\n        return info['path']\n\n    def load_image(self, image_id):\n        info = self.image_info[image_id]\n        fp = info['path']\n        image = imread(fp)\n        # If grayscale. Convert to RGB for consistency.\n        if len(image.shape) != 3 or image.shape[2] != 3:\n            image = np.stack((image,) * 3, -1)\n        return image\n\n    def load_mask(self, image_id):\n        info = self.image_info[image_id]\n        annotations = info['annotations']\n#         print(image_id, annotations)\n        count = len(annotations)\n        if count == 0:\n            mask = np.zeros((info['orig_height'], info['orig_width'], 1), dtype=np.uint8)\n            class_ids = np.zeros((1,), dtype=np.int32)\n        else:\n            mask = np.zeros((info['orig_height'], info['orig_width'], count), dtype=np.uint8)\n            class_ids = np.zeros((count,), dtype=np.int32)\n            for i, a in enumerate(annotations):\n                mask[:, :, i] = rle_decode(a)\n                class_ids[i] = 1\n        return mask.astype(np.bool), class_ids.astype(np.int32)","metadata":{"id":"8EBVA1M60yAj","_uuid":"52bd3ffbdde0173a363055482d675da51c2aba99","execution":{"iopub.status.busy":"2022-04-14T17:32:04.876637Z","iopub.execute_input":"2022-04-14T17:32:04.876879Z","iopub.status.idle":"2022-04-14T17:32:04.896242Z","shell.execute_reply.started":"2022-04-14T17:32:04.876834Z","shell.execute_reply":"2022-04-14T17:32:04.892348Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":"* ### Take a look at an image with a ship on it","metadata":{"id":"9RlMo04ckd98","_uuid":"1cb852e262b69d348743767d675573368ab672c9"}},{"cell_type":"code","source":"ds = imread(os.path.join(train_dir, image_fps[0])) # read  image from filepath \nplt.imshow(ds)\n\n# Original image size: 768 x 768\nORIG_SIZE = ds.shape[0]\nprint(\"Image original size:\",ORIG_SIZE)","metadata":{"id":"YPqjEIXWRhSf","_uuid":"6c386dcef041b972f6209dd19e247d547c3c349f","execution":{"iopub.status.busy":"2022-04-14T17:32:04.897654Z","iopub.execute_input":"2022-04-14T17:32:04.898255Z","iopub.status.idle":"2022-04-14T17:32:05.200587Z","shell.execute_reply.started":"2022-04-14T17:32:04.898096Z","shell.execute_reply":"2022-04-14T17:32:05.199661Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"* ### Format the training and validation datasets for Matterport Mask-RCNN","metadata":{"id":"9KUvacUbgiEX","_uuid":"a5143c19dc22bc00d318a3b28cb7e13c7fbacc8a"}},{"cell_type":"code","source":"%%time\n# prepare the training dataset\ndataset_train = DetectorDataset(image_fps_train, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_train.prepare()","metadata":{"id":"jwMkhotP0yFf","_uuid":"86c3333d4dfb8b7d00ce1f401693d0df4e6254e1","execution":{"iopub.status.busy":"2022-04-14T17:32:05.201940Z","iopub.execute_input":"2022-04-14T17:32:05.205257Z","iopub.status.idle":"2022-04-14T17:32:06.192579Z","shell.execute_reply.started":"2022-04-14T17:32:05.205011Z","shell.execute_reply":"2022-04-14T17:32:06.191776Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"%%time\n# prepare the validation dataset\ndataset_val = DetectorDataset(image_fps_val, image_annotations, ORIG_SIZE, ORIG_SIZE)\ndataset_val.prepare()","metadata":{"id":"K1TkWuGP0yHl","_uuid":"313347d838fa8321a714858c8073f98c50c5be26","execution":{"iopub.status.busy":"2022-04-14T17:32:06.193766Z","iopub.execute_input":"2022-04-14T17:32:06.194202Z","iopub.status.idle":"2022-04-14T17:32:07.079148Z","shell.execute_reply.started":"2022-04-14T17:32:06.194154Z","shell.execute_reply":"2022-04-14T17:32:07.078359Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"* ### Display a random image with bounding boxes","metadata":{"id":"pEXEt8fygWuC","_uuid":"600a8135d4e382f62797d69e9358f5697873c8f9"}},{"cell_type":"code","source":"# Load and display random sample and their bounding boxes\n\nclass_ids = [0]\nwhile class_ids[0] == 0:  ## look for a mask\n    image_id = random.choice(dataset_val.image_ids) # random image id 0-90\n    image_fp = dataset_val.image_reference(image_id) # current image\n    image = dataset_val.load_image(image_id) # load image\n    mask, class_ids = dataset_val.load_mask(image_id)\nprint(\"image shape:\",image.shape)\nprint(\"mask shape :\",mask.shape)\nprint(\"image file:\",image_fp, \"class_id:\",class_ids)\n\nplt.figure(figsize=(10, 10))\nplt.subplot(1, 2, 1)\nplt.imshow(image)\nplt.axis('off')\n\nplt.subplot(1, 2, 2)\nmasked = np.zeros(image.shape[:2])\nfor i in range(mask.shape[2]):\n    masked += mask[:, :, i] ## * image[:, :, 0]\nplt.imshow(masked, cmap='gray')\nplt.axis('off')","metadata":{"id":"4xwsrf9G1lHR","outputId":"a13386d3-a918-41fe-8824-13625c9d7b08","_uuid":"491b78ec96d28fcdbbf8e2d7f9320a05d64c9249","execution":{"iopub.status.busy":"2022-04-14T17:32:07.080467Z","iopub.execute_input":"2022-04-14T17:32:07.080796Z","iopub.status.idle":"2022-04-14T17:32:07.434429Z","shell.execute_reply.started":"2022-04-14T17:32:07.080746Z","shell.execute_reply":"2022-04-14T17:32:07.433390Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"* ### Image Augmentation used to mutate images to increase number of input images for training","metadata":{"id":"ustAIH78hZI_","_uuid":"342b6008873fe7a6a0870a712ee47a87f0d2828d"}},{"cell_type":"code","source":"# Image augmentation (light but constant)\naugmentation = iaa.Sequential([\n    iaa.OneOf([ ## rotate\n        iaa.Affine(rotate=0),\n        iaa.Affine(rotate=90),\n        iaa.Affine(rotate=180),\n        iaa.Affine(rotate=270),\n    ]),\n    iaa.Fliplr(0.5),\n    iaa.Flipud(0.5),\n    iaa.OneOf([ ## brightness or contrast\n        iaa.Multiply((0.9, 1.1)),\n        iaa.ContrastNormalization((0.9, 1.1)),\n    ]),\n    iaa.OneOf([ ## blur or sharpen\n        iaa.GaussianBlur(sigma=(0.0, 0.1)),\n        iaa.Sharpen(alpha=(0.0, 0.1)),\n    ]),\n])\n\n# test on the same image as above\nimggrid = augmentation.draw_grid(image, cols=4, rows=2)\nplt.figure(figsize=(15, 15))\nplt.imshow(imggrid.astype(int))","metadata":{"id":"STZnQTE61lME","_uuid":"4ab9d6086ce611a46f189c047956c43b29783e6d","execution":{"iopub.status.busy":"2022-04-14T17:32:07.435603Z","iopub.execute_input":"2022-04-14T17:32:07.435942Z","iopub.status.idle":"2022-04-14T17:32:08.923245Z","shell.execute_reply.started":"2022-04-14T17:32:07.435891Z","shell.execute_reply":"2022-04-14T17:32:08.922558Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"## Step Three - Training the model\nOn a P100 GPU debug size takes about 10 minutes, non-debug about 90 minutes\n\nNote: the following model is for demonstration purpose only. We have limited the training to one epoch, and have set nominal values for the Detector Configuration to reduce run-time. \n\n- dataset_train and dataset_val are derived from DetectorDataset \n- DetectorDataset loads images from image filenames and  masks from the annotation data\n- model is Mask-RCNN","metadata":{"id":"M4kt7LKuc78e","_uuid":"7e65d2cecb283f446f34cdde19b663a8a8e9590f"}},{"cell_type":"code","source":"model = modellib.MaskRCNN(mode='training', config=config, model_dir=ROOT_DIR) #  ROOT_DIR:output path\n\n# Exclude the last layers because they require a matching\n# number of classes\nmodel.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n    \"mrcnn_bbox\", \"mrcnn_mask\"])","metadata":{"_uuid":"138d6197fc8dce9f1f8a7b5a6c27aa2069698e03","execution":{"iopub.status.busy":"2022-04-14T17:32:08.924418Z","iopub.execute_input":"2022-04-14T17:32:08.925015Z","iopub.status.idle":"2022-04-14T17:32:20.379513Z","shell.execute_reply.started":"2022-04-14T17:32:08.924954Z","shell.execute_reply":"2022-04-14T17:32:20.378750Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"LEARNING_RATE = 0.003\n\n# Train Mask-RCNN Model \nimport warnings \nwarnings.filterwarnings(\"ignore\")","metadata":{"id":"RVgNhHjl1lOS","outputId":"2cba9efc-eeea-472d-d155-3c3d856585bf","_uuid":"64cce2581ffdb8c2b1cb07948ada4a93f64874b0","execution":{"iopub.status.busy":"2022-04-14T17:32:20.380442Z","iopub.execute_input":"2022-04-14T17:32:20.380702Z","iopub.status.idle":"2022-04-14T17:32:20.385364Z","shell.execute_reply.started":"2022-04-14T17:32:20.380657Z","shell.execute_reply":"2022-04-14T17:32:20.384569Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"code","source":"%%time\n## train heads with higher lr to speedup the learning  2*LEARNING_RATE\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE*2,\n            epochs=2,\n            layers='heads',\n            augmentation=None)  ## no need to augment yet\n\nhistory = model.keras_model.history.history","metadata":{"_uuid":"cf339a499519d174bcdf2311a1802f0e3acb1758","execution":{"iopub.status.busy":"2022-04-14T17:32:20.386539Z","iopub.execute_input":"2022-04-14T17:32:20.387003Z","iopub.status.idle":"2022-04-14T17:37:37.453912Z","shell.execute_reply.started":"2022-04-14T17:32:20.386951Z","shell.execute_reply":"2022-04-14T17:37:37.449405Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"%%time\n # LEARNING_RATE\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE,\n            epochs=4 if debug else 14,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","metadata":{"_uuid":"8004790d27f041793562e994bbe95edf67f8978b","execution":{"iopub.status.busy":"2022-04-14T17:37:37.458990Z","iopub.execute_input":"2022-04-14T17:37:37.464262Z","iopub.status.idle":"2022-04-14T17:42:42.257010Z","shell.execute_reply.started":"2022-04-14T17:37:37.464192Z","shell.execute_reply":"2022-04-14T17:42:42.255033Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"%%time\n### LEARNING_RATE/2 fine tuning işleminde overfiting olmaması için lr traine göre düşürülür.\nmodel.train(dataset_train, dataset_val,\n            learning_rate=LEARNING_RATE/2,\n            epochs=6 if debug else 22,\n            layers='all',\n            augmentation=augmentation)\n\nnew_history = model.keras_model.history.history\nfor k in new_history: history[k] = history[k] + new_history[k]","metadata":{"_uuid":"e0f55437aaa49e58ae60225a035fa8a3f6b604d3","execution":{"iopub.status.busy":"2022-04-14T17:42:42.264963Z","iopub.execute_input":"2022-04-14T17:42:42.265996Z","iopub.status.idle":"2022-04-14T17:48:19.476003Z","shell.execute_reply.started":"2022-04-14T17:42:42.265368Z","shell.execute_reply":"2022-04-14T17:48:19.473956Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"epochs = range(1, len(history['loss'])+1)\npd.DataFrame(history, index=epochs)","metadata":{"_uuid":"71abf32327a102e1c22e944b24d98690c71d9560","execution":{"iopub.status.busy":"2022-04-14T17:48:19.483866Z","iopub.execute_input":"2022-04-14T17:48:19.491385Z","iopub.status.idle":"2022-04-14T17:48:19.582222Z","shell.execute_reply.started":"2022-04-14T17:48:19.484253Z","shell.execute_reply":"2022-04-14T17:48:19.581071Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(21,11))\n\nplt.subplot(231)\nplt.plot(epochs, history[\"loss\"], label=\"Train loss\")\nplt.plot(epochs, history[\"val_loss\"], label=\"Valid loss\")\nplt.legend()\nplt.subplot(232)\nplt.plot(epochs, history[\"rpn_class_loss\"], label=\"Train RPN class ce\")\nplt.plot(epochs, history[\"val_rpn_class_loss\"], label=\"Valid RPN class ce\")\nplt.legend()\nplt.subplot(233)\nplt.plot(epochs, history[\"rpn_bbox_loss\"], label=\"Train RPN box loss\")\nplt.plot(epochs, history[\"val_rpn_bbox_loss\"], label=\"Valid RPN box loss\")\nplt.legend()\nplt.subplot(234)\nplt.plot(epochs, history[\"mrcnn_class_loss\"], label=\"Train MRCNN class ce\")\nplt.plot(epochs, history[\"val_mrcnn_class_loss\"], label=\"Valid MRCNN class ce\")\nplt.legend()\nplt.subplot(235)\nplt.plot(epochs, history[\"mrcnn_bbox_loss\"], label=\"Train MRCNN box loss\")\nplt.plot(epochs, history[\"val_mrcnn_bbox_loss\"], label=\"Valid MRCNN box loss\")\nplt.legend()\nplt.subplot(236)\nplt.plot(epochs, history[\"mrcnn_mask_loss\"], label=\"Train Mask loss\")\nplt.plot(epochs, history[\"val_mrcnn_mask_loss\"], label=\"Valid Mask loss\")\nplt.legend()\n\nplt.show()","metadata":{"_uuid":"fb3b69242b91dcc49697ff076ceeb957347372e1","execution":{"iopub.status.busy":"2022-04-14T17:48:19.586736Z","iopub.execute_input":"2022-04-14T17:48:19.590406Z","iopub.status.idle":"2022-04-14T17:48:23.038172Z","shell.execute_reply.started":"2022-04-14T17:48:19.590357Z","shell.execute_reply":"2022-04-14T17:48:23.037106Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"best_epoch = np.argmin(history[\"val_loss\"])\nscore = history[\"val_loss\"][best_epoch]\nprint(f'Best Epoch:{best_epoch+1} val_loss:{score}')","metadata":{"_uuid":"5c2b38ecbc84575295dd62657ed175c5a0b72021","execution":{"iopub.status.busy":"2022-04-14T17:48:23.044053Z","iopub.execute_input":"2022-04-14T17:48:23.046016Z","iopub.status.idle":"2022-04-14T17:48:23.060042Z","shell.execute_reply.started":"2022-04-14T17:48:23.045954Z","shell.execute_reply":"2022-04-14T17:48:23.058770Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"# select trained model \ndir_names = next(os.walk(model.model_dir))[1]\nkey = config.NAME.lower()\ndir_names = filter(lambda f: f.startswith(key), dir_names)\ndir_names = sorted(dir_names)\n\nif not dir_names:\n    import errno\n    raise FileNotFoundError(\n        errno.ENOENT,\n        \"Could not find model directory under {}\".format(self.model_dir))\n\nfps = []\n# Pick last directory\nfor d in dir_names: \n    dir_name = os.path.join(model.model_dir, d)\n    # Find the last checkpoint\n    checkpoints = next(os.walk(dir_name))[2]\n    checkpoints = filter(lambda f: f.startswith(\"mask_rcnn\"), checkpoints)\n    checkpoints = sorted(checkpoints)\n    if not checkpoints:\n        print('No weight files in {}'.format(dir_name))\n    else:\n        checkpoint = os.path.join(dir_name, checkpoints[best_epoch])\n        fps.append(checkpoint)\n\nmodel_path = sorted(fps)[-1]\nprint('Found model {}'.format(model_path))","metadata":{"id":"eraRlzgPmmIZ","outputId":"de9e688c-ba4f-4b62-f842-dbcf00ce397c","_uuid":"db5c10d3f7da099e5751a04a6e6d49819882ecd4","execution":{"iopub.status.busy":"2022-04-14T17:48:23.061820Z","iopub.execute_input":"2022-04-14T17:48:23.062581Z","iopub.status.idle":"2022-04-14T17:48:23.098669Z","shell.execute_reply.started":"2022-04-14T17:48:23.062522Z","shell.execute_reply":"2022-04-14T17:48:23.096850Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"## Step Four - Run the Test set and submit","metadata":{}},{"cell_type":"code","source":"class InferenceConfig(DetectorConfig):\n    GPU_COUNT = 1\n    IMAGES_PER_GPU = 1\n\ninference_config = InferenceConfig()\n\n# Recreate the model in inference mode\nmodel = modellib.MaskRCNN(mode='inference', \n                          config=inference_config,\n                          model_dir=ROOT_DIR)\n\n# Load trained weights (fill in path to trained weights here)\nassert model_path != \"\", \"Provide path to trained weights\"\nprint(\"Loading weights from \", model_path)\nmodel.load_weights(model_path, by_name=True)","metadata":{"id":"TgpT9AzC2Bgz","outputId":"60f5a175-4666-497d-b4e8-0bdab39a92d0","_uuid":"52138636b2ae5bf444bba808518cd8313bde65cd","execution":{"iopub.status.busy":"2022-04-14T17:48:23.101705Z","iopub.execute_input":"2022-04-14T17:48:23.103202Z","iopub.status.idle":"2022-04-14T17:48:32.281150Z","shell.execute_reply.started":"2022-04-14T17:48:23.102942Z","shell.execute_reply":"2022-04-14T17:48:32.280201Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# set color for class\ndef get_colors_for_class_ids(class_ids):\n    colors = []\n    for class_id in class_ids:\n        if class_id == 1:\n            colors.append((.941, .204, .204))\n    return colors","metadata":{"id":"9mTBig7D2BjU","_uuid":"e13c61bee23b791c61ecf1256f7512295cd4d9ab","execution":{"iopub.status.busy":"2022-04-14T17:48:32.282420Z","iopub.execute_input":"2022-04-14T17:48:32.282908Z","iopub.status.idle":"2022-04-14T17:48:32.288938Z","shell.execute_reply.started":"2022-04-14T17:48:32.282853Z","shell.execute_reply":"2022-04-14T17:48:32.288178Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"* ### How does the predicted box compared to the expected value? Let's use the validation dataset to check. ","metadata":{"id":"A8EiL2LOiCr_","_uuid":"f99fbd3f31ff1a2bd66764835c9b646375364598"}},{"cell_type":"code","source":"# Show few example of ground truth vs. predictions on the validation dataset \ndataset = dataset_val\nfig = plt.figure(figsize=(10, 40))\n\nfor i in range(8):\n\n    image_id = random.choice(dataset.image_ids)\n    \n    original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n        modellib.load_image_gt(dataset_val, inference_config, \n                               image_id, use_mini_mask=False)\n    \n#     print(original_image.shape)\n    plt.subplot(8, 2, 2*i + 1)\n    visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n                                dataset.class_names,\n                                colors=get_colors_for_class_ids(gt_class_id), ax=fig.axes[-1])\n    \n    plt.subplot(8, 2, 2*i + 2)\n    results = model.detect([original_image]) #, verbose=1)\n    r = results[0]\n    visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n                                dataset.class_names, r['scores'], \n                                colors=get_colors_for_class_ids(r['class_ids']), ax=fig.axes[-1])","metadata":{"id":"irheTbrW2Bl0","outputId":"56041ad4-173d-45ab-af67-f54e8333511e","_uuid":"186412199e25b98719f71cfe5e8869abcce516c4","execution":{"iopub.status.busy":"2022-04-14T17:48:32.290307Z","iopub.execute_input":"2022-04-14T17:48:32.290859Z","iopub.status.idle":"2022-04-14T17:48:38.444105Z","shell.execute_reply.started":"2022-04-14T17:48:32.290810Z","shell.execute_reply":"2022-04-14T17:48:38.443341Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"* ### Final steps - Create the filtered submission file","metadata":{"id":"WcV1cL_aiSc4","_uuid":"164e18701a830bc6c42a791feea13549de37289b"}},{"cell_type":"code","source":"# Get filenames of test dataset images\ntest_image_fps = test_names","metadata":{"_uuid":"5a124f21c2918ac4cb40ce99c852b86ea223d7e4","execution":{"iopub.status.busy":"2022-04-14T17:48:38.445171Z","iopub.execute_input":"2022-04-14T17:48:38.445569Z","iopub.status.idle":"2022-04-14T17:48:38.449378Z","shell.execute_reply.started":"2022-04-14T17:48:38.445508Z","shell.execute_reply":"2022-04-14T17:48:38.448723Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"DETECTION_TEST_PRED = '/kaggle/input/fine-tuning-resnet34-on-ship-detection-new-data/ship_detection.csv'\nship_detection = pd.read_csv(DETECTION_TEST_PRED, index_col='id')\nship_detection.head()","metadata":{"_uuid":"a5b7afbfbcde9afe9ef3d80263ad7f55a85531f0","execution":{"iopub.status.busy":"2022-04-14T17:48:38.450402Z","iopub.execute_input":"2022-04-14T17:48:38.450831Z","iopub.status.idle":"2022-04-14T17:48:38.494797Z","shell.execute_reply.started":"2022-04-14T17:48:38.450778Z","shell.execute_reply":"2022-04-14T17:48:38.493889Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"THRESHOLD = 0.45\ntest_names_nothing = ship_detection.loc[ship_detection['p_ship'] <= THRESHOLD].index.tolist()\nlen(test_names_nothing), len(ship_detection), len(test_names_nothing)/len(ship_detection)","metadata":{"_uuid":"02321d63a0610d91f9dae621002dc7d4cce57034","execution":{"iopub.status.busy":"2022-04-14T17:48:38.496116Z","iopub.execute_input":"2022-04-14T17:48:38.496801Z","iopub.status.idle":"2022-04-14T17:48:38.511580Z","shell.execute_reply.started":"2022-04-14T17:48:38.496610Z","shell.execute_reply":"2022-04-14T17:48:38.510720Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Make predictions on test images, write out sample submission\ndef predict(image_fps, filepath='submission.csv', min_conf=config.DETECTION_MIN_CONFIDENCE):\n    # assume square image\n    resize_factor = ORIG_SIZE / config.IMAGE_SHAPE[0]\n    #resize_factor = ORIG_SIZE\n    with open(filepath, 'w') as file:\n        file.write(\"ImageId,EncodedPixels\\n\")\n\n        for image_id in tqdm(image_fps):\n            found = False\n            \n            if image_id not in test_names_nothing:\n                image = imread(os.path.join(test_dir, image_id))\n                # If grayscale. Convert to RGB for consistency.\n                if len(image.shape) != 3 or image.shape[2] != 3:\n                    image = np.stack((image,) * 3, -1)\n#                 image, window, scale, padding, crop = utils.resize_image(\n#                     image,\n#                     min_dim=config.IMAGE_MIN_DIM,\n#                     min_scale=config.IMAGE_MIN_SCALE,\n#                     max_dim=config.IMAGE_MAX_DIM,\n#                     mode=config.IMAGE_RESIZE_MODE)\n\n                results = model.detect([image])\n                r = results[0]\n\n                assert( len(r['rois']) == len(r['class_ids']) == len(r['scores']) )\n                if len(r['rois']) == 0:\n                    pass  ## no ship\n                else:\n                    num_instances = len(r['rois'])\n\n                    for i in range(num_instances):\n                        if r['scores'][i] > min_conf:\n#                             print(r['scores'][i], r['rois'][i], r['masks'].shape, np.sum(r['masks'][...,i]))\n#                             plt.imshow(r['masks'][...,i], cmap=get_cmap('jet'))\n                            file.write(image_id + \",\" + rle_encode(r['masks'][...,i]) + \"\\n\")\n                            found = True\n\n            if not found:\n                file.write(image_id + \",\\n\")  ## no ship","metadata":{"id":"C6UWVrbM2Bob","_uuid":"4a5c0c6134408ddbf5a34496d7e9d7be5692e9a1","execution":{"iopub.status.busy":"2022-04-14T17:48:38.513050Z","iopub.execute_input":"2022-04-14T17:48:38.513448Z","iopub.status.idle":"2022-04-14T17:48:38.529091Z","shell.execute_reply.started":"2022-04-14T17:48:38.513297Z","shell.execute_reply":"2022-04-14T17:48:38.528392Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"submission_fp = os.path.join(ROOT_DIR, 'submission.csv')\npredict(test_image_fps, filepath=submission_fp)\nprint(submission_fp)","metadata":{"id":"C5cBpNka2Bsv","outputId":"a2af9176-d9d6-49f6-f22a-5a1c455d144f","_uuid":"0406e7f5aaa4867782c4f9c064f90bba386128e7","execution":{"iopub.status.busy":"2022-04-14T17:48:38.536433Z","iopub.execute_input":"2022-04-14T17:48:38.537005Z","iopub.status.idle":"2022-04-14T17:48:42.112077Z","shell.execute_reply.started":"2022-04-14T17:48:38.536955Z","shell.execute_reply":"2022-04-14T17:48:42.111029Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"sub = pd.read_csv(submission_fp)\nprint(sub.EncodedPixels.isnull().sum(), sub.ImageId.nunique(), sub.EncodedPixels.isnull().sum()/sub.ImageId.nunique())\nsub.head(50)","metadata":{"id":"_BjPE_Ee9rbA","outputId":"67b5f053-112b-494a-9ab3-d017bfb440c2","_uuid":"3fd8d178fc51ef0bca94fbb3f423160f08a77edc","execution":{"iopub.status.busy":"2022-04-14T17:48:42.114182Z","iopub.execute_input":"2022-04-14T17:48:42.114634Z","iopub.status.idle":"2022-04-14T17:48:42.140279Z","shell.execute_reply.started":"2022-04-14T17:48:42.114440Z","shell.execute_reply":"2022-04-14T17:48:42.139541Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"# show a few test image detection example\ndef visualize_test(): \n    image_id = random.choice(test_names)\n    \n    # original image\n#     print(image_id)\n    image = imread(os.path.join(test_dir, image_id))\n    \n    # assume square image \n    resize_factor = 1 ## ORIG_SIZE / config.IMAGE_SHAPE[0]\n    \n    # If grayscale. Convert to RGB for consistency.\n    if len(image.shape) != 3 or image.shape[2] != 3:\n        image = np.stack((image,) * 3, -1) \n#     image, window, scale, padding, crop = utils.resize_image(\n#         image,\n#         min_dim=config.IMAGE_MIN_DIM,\n#         min_scale=config.IMAGE_MIN_SCALE,\n#         max_dim=config.IMAGE_MAX_DIM,\n#         mode=config.IMAGE_RESIZE_MODE)\n\n    results = model.detect([image])\n    r = results[0]\n    for bbox in r['rois']: \n#         print(bbox)\n        x1 = int(bbox[1] * resize_factor)\n        y1 = int(bbox[0] * resize_factor)\n        x2 = int(bbox[3] * resize_factor)\n        y2 = int(bbox[2]  * resize_factor)\n        cv2.rectangle(image, (x1,y1), (x2,y2), (77, 255, 9), 3, 1)\n        width = x2 - x1 \n        height = y2 - y1 \n#         print(\"x {} y {} h {} w {}\".format(x1, y1, width, height))\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n    ax1.set_title(f\"{image_id}\")\n    ax1.imshow(image)\n    ax2.set_title(f\"{len(r['rois'])} masks if prob:{ship_detection.loc[image_id][0]:.6f}\")\n    ax2.imshow(masks_as_color(sub.query(f\"ImageId=='{image_id}'\")['EncodedPixels']))\n\nfor i in range(8):\n    visualize_test()","metadata":{"_uuid":"ea110f197abc2acb1c3435383f7259079dc0eb0e","execution":{"iopub.status.busy":"2022-04-14T17:48:42.141498Z","iopub.execute_input":"2022-04-14T17:48:42.142053Z","iopub.status.idle":"2022-04-14T17:48:47.875605Z","shell.execute_reply.started":"2022-04-14T17:48:42.141995Z","shell.execute_reply":"2022-04-14T17:48:47.874661Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"# Save a local copy of the submission file just in case\n!cd /kaggle/working\nfrom IPython.display import FileLink\nFileLink(r'../submission.csv')","metadata":{"execution":{"iopub.status.busy":"2022-04-14T17:51:23.949116Z","iopub.execute_input":"2022-04-14T17:51:23.949438Z","iopub.status.idle":"2022-04-14T17:51:24.752266Z","shell.execute_reply.started":"2022-04-14T17:51:23.949381Z","shell.execute_reply":"2022-04-14T17:51:24.751380Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"# remove files to allow committing (hit files limit otherwise)\n!rm -rf /kaggle/working/Mask_RCNN","metadata":{"_uuid":"835a15c9d018acd5deb16e9e02f9b765f68d0e78","execution":{"iopub.status.busy":"2022-04-14T17:48:47.885864Z","iopub.execute_input":"2022-04-14T17:48:47.886361Z","iopub.status.idle":"2022-04-14T17:48:48.810734Z","shell.execute_reply.started":"2022-04-14T17:48:47.886106Z","shell.execute_reply":"2022-04-14T17:48:48.809761Z"},"trusted":true},"execution_count":82,"outputs":[]}]}